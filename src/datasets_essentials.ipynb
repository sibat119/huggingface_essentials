{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading an existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\")\n",
    "# Show an example data point\n",
    "dataset['train'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': Value(dtype='string', id=None), 'label': ClassLabel(names=['neg', 'pos'], id=None)}\n",
      "{'text': ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.', \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\", \"This film was probably inspired by Godard's Masculin, féminin and I urge you to see that film instead.<br /><br />The film has two strong elements and those are, (1) the realistic acting (2) the impressive, undeservedly good, photo. Apart from that, what strikes me most is the endless stream of silliness. Lena Nyman has to be most annoying actress in the world. She acts so stupid and with all the nudity in this film,...it's unattractive. Comparing to Godard's film, intellectuality has been replaced with stupidity. Without going too far on this subject, I would say that follows from the difference in ideals between the French and the Swedish society.<br /><br />A movie of its time, and place. 2/10.\", 'Oh, brother...after hearing about this ridiculous film for umpteen years all I can think of is that old Peggy Lee song..<br /><br />\"Is that all there is??\" ...I was just an early teen when this smoked fish hit the U.S. I was too young to get in the theater (although I did manage to sneak into \"Goodbye Columbus\"). Then a screening at a local film museum beckoned - Finally I could see this film, except now I was as old as my parents were when they schlepped to see it!!<br /><br />The ONLY reason this film was not condemned to the anonymous sands of time was because of the obscenity case sparked by its U.S. release. MILLIONS of people flocked to this stinker, thinking they were going to see a sex film...Instead, they got lots of closeups of gnarly, repulsive Swedes, on-street interviews in bland shopping malls, asinie political pretension...and feeble who-cares simulated sex scenes with saggy, pale actors.<br /><br />Cultural icon, holy grail, historic artifact..whatever this thing was, shred it, burn it, then stuff the ashes in a lead box!<br /><br />Elite esthetes still scrape to find value in its boring pseudo revolutionary political spewings..But if it weren\\'t for the censorship scandal, it would have been ignored, then forgotten.<br /><br />Instead, the \"I Am Blank, Blank\" rhythymed title was repeated endlessly for years as a titilation for porno films (I am Curious, Lavender - for gay films, I Am Curious, Black - for blaxploitation films, etc..) and every ten years or so the thing rises from the dead, to be viewed by a new generation of suckers who want to see that \"naughty sex film\" that \"revolutionized the film industry\"...<br /><br />Yeesh, avoid like the plague..Or if you MUST see it - rent the video and fast forward to the \"dirty\" parts, just to get it over with.<br /><br />'], 'label': [0, 0, 0, 0, 0]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the Dataset\n",
    "# How to explore the dataset structure (e.g., columns, data types).\n",
    "# Show the structure\n",
    "print(dataset['train'].features)\n",
    "\n",
    "# Display first five entries\n",
    "print(dataset['train'][:5])\n",
    "dataset['train'] = dataset['train'].select(range(100))\n",
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load a custom dataset using script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14041\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3250\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3453\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_dataset = datasets.load_dataset(path=\"conll.py\", trust_remote_code=True)\n",
    "ner_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load a custom dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for token classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            words: list. The words of the sequence.\n",
    "            labels: (Optional) list. The labels for each word of the sequence. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "HP_SEARCH_DATA_SIZE = 128\n",
    "import random\n",
    "import functools\n",
    "\n",
    "def read_examples_from_file(data_dir, mode, batch_size, is_hyper_parameter_search = False):\n",
    "    total_examples = []\n",
    "    breakpoint()\n",
    "    examples = []\n",
    "    file_path = data_dir\n",
    "    print(file_path)\n",
    "    guid_index = 1\n",
    "    \n",
    "    with open(file_path, encoding=\"utf-8\") as f:\n",
    "        words = []\n",
    "        labels = []\n",
    "        for line in f:\n",
    "            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n",
    "                if words:\n",
    "                    examples.append(InputExample(guid=\"{}-{}\".format(mode, guid_index),\n",
    "                                                words=words,\n",
    "                                                labels=labels))\n",
    "                    guid_index += 1\n",
    "                    words = []\n",
    "                    labels = []\n",
    "            else:\n",
    "                splits = line.split(\" \")\n",
    "                words.append(splits[0].lower())\n",
    "                if len(splits) > 1:\n",
    "                    labels.append(splits[-1].replace(\"\\n\", \"\").strip())\n",
    "                else:\n",
    "                    # Examples could have no label for mode = \"test\"\n",
    "                    labels.append(\"O\")\n",
    "            if guid_index > 4 * HP_SEARCH_DATA_SIZE:\n",
    "                break\n",
    "        if words:\n",
    "            examples.append(InputExample(guid=\"%s-%d\".format(mode, guid_index),\n",
    "                                        words=words,\n",
    "                                        labels=labels))\n",
    "            \n",
    "\n",
    "        total_examples.append(examples)\n",
    "    min_len = min([len(ex) for ex in total_examples])\n",
    "    [random.shuffle(ex) for ex in total_examples]\n",
    "    total_examples = [ex[:min_len] for ex in total_examples]\n",
    "    total_examples = functools.reduce(lambda a, b: a+b, total_examples)\n",
    "    extra_example_size = (len(total_examples) % batch_size)\n",
    "    print(\"Number of examples: {}\".format(len(total_examples[:-(len(total_examples) % batch_size)])))\n",
    "    random.shuffle(total_examples)\n",
    "    if is_hyper_parameter_search:\n",
    "        total_examples = total_examples[:HP_SEARCH_DATA_SIZE]\n",
    "        return total_examples\n",
    "    return total_examples if extra_example_size == 0 else total_examples[:-(len(total_examples) % batch_size)]\n",
    "\n",
    "\n",
    "def convert_examples_to_features(examples,\n",
    "                                 label_list,\n",
    "                                 max_seq_length,\n",
    "                                 tokenizer,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token=\"[CLS]\",\n",
    "                                 cls_token_segment_id=1,\n",
    "                                 sep_token=\"[SEP]\",\n",
    "                                 sep_token_extra=False,\n",
    "                                 pad_on_left=False,\n",
    "                                 pad_token=0,\n",
    "                                 pad_token_segment_id=0,\n",
    "                                 pad_token_label_id=-1,\n",
    "                                 sequence_a_segment_id=0,\n",
    "                                 mask_padding_with_zero=True\n",
    "                                ):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "    print('label map', label_map)\n",
    "\n",
    "    features = []\n",
    "    # examples = examples[:10000]\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        #if ex_index % 10000 == 0:\n",
    "        #    print(\"Writing example %d of %d\", ex_index, len(examples))\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        i = 0\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            if label == '' or label == ' ' or word == '' or word == ' ':\n",
    "                continue\n",
    "            i += 1\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            tokens.extend(word_tokens)\n",
    "            # print(word)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids:   0   0   0   0  0     0   0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        padding_length_label = max_seq_length - len(label_ids)\n",
    "        # print(padding_length, padding_length_label)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length_label) + label_ids\n",
    "        else:\n",
    "            input_ids += ([pad_token] * padding_length)\n",
    "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
    "            # print(len(label_ids))\n",
    "            label_ids += ([pad_token_label_id] * padding_length_label)\n",
    "            # print(len(label_ids))\n",
    "\n",
    "        # print('\\n\\n\\nMax seq len:', max_seq_length, 'Label ids len',  len(label_ids))\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        if ex_index < 5:\n",
    "           print(\"*** Example ***\")\n",
    "           print(\"guid: %s\", example.guid)\n",
    "           print(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n",
    "           print(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n",
    "           print(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n",
    "           print(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n",
    "           print(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=label_ids))\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/conll_data.txt\n",
      "Number of examples: 336\n"
     ]
    }
   ],
   "source": [
    "example_path = \"../data/conll_data.txt\"\n",
    "\n",
    "examples = read_examples_from_file(\n",
    "    data_dir=example_path, \n",
    "    mode=\"train\",\n",
    "    batch_size=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label map {'O': 0, 'I-AGG': 1, 'I-ATR': 2, 'I-ENT': 3, 'I-FLT': 4, 'I-FLO': 5, 'I-NUM': 6, 'I-PRW': 7}\n",
      "*** Example ***\n",
      "guid: %s train-2\n",
      "tokens: %s [CLS] predict the average airline delay for each airline tomorrow [SEP]\n",
      "input_ids: %s 101 17163 1103 1903 8694 8513 1111 1296 8694 4911 102 0 0 0 0 0\n",
      "input_mask: %s 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0\n",
      "segment_ids: %s 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label_ids: %s -1 0 0 1 2 2 0 0 3 7 -1 -1 -1 -1 -1 -1\n",
      "*** Example ***\n",
      "guid: %s train-34\n",
      "tokens: %s [CLS] predict the average air system delay for flights for each destination airport which will [SEP]\n",
      "input_ids: %s 101 17163 1103 1903 1586 1449 8513 1111 7306 1111 1296 7680 3871 1134 1209 102\n",
      "input_mask: %s 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "segment_ids: %s 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label_ids: %s -1 0 0 1 2 2 2 0 0 0 0 3 3 0 0 -1\n",
      "*** Example ***\n",
      "guid: %s train-332\n",
      "tokens: %s [CLS] predict the average change is duration of flights for flights which will land in [SEP]\n",
      "input_ids: %s 101 17163 1103 1903 1849 1110 9355 1104 7306 1111 7306 1134 1209 1657 1107 102\n",
      "input_mask: %s 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "segment_ids: %s 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label_ids: %s -1 0 0 1 0 5 2 2 2 0 0 0 0 0 0 -1\n",
      "*** Example ***\n",
      "guid: %s train-116\n",
      "tokens: %s [CLS] predict the average arrival delay for flights for each origin airport which will have [SEP]\n",
      "input_ids: %s 101 17163 1103 1903 4870 8513 1111 7306 1111 1296 4247 3871 1134 1209 1138 102\n",
      "input_mask: %s 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "segment_ids: %s 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label_ids: %s -1 0 0 1 2 2 0 0 0 0 3 3 0 0 0 -1\n",
      "*** Example ***\n",
      "guid: %s train-49\n",
      "tokens: %s [CLS] predict the total airline delay for flights for each destination airport with security delay [SEP]\n",
      "input_ids: %s 101 17163 1103 1703 8694 8513 1111 7306 1111 1296 7680 3871 1114 2699 8513 102\n",
      "input_mask: %s 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      "segment_ids: %s 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "label_ids: %s -1 0 0 1 2 2 0 0 0 0 3 3 0 4 4 -1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sibat/miniconda3/envs/hfess/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from torch.nn import CrossEntropyLoss\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "pad_token_label_id = CrossEntropyLoss().ignore_index\n",
    "labels = ['O','I-AGG','I-ATR','I-ENT','I-FLT','I-FLO','I-NUM','I-PRW']\n",
    "features = convert_examples_to_features(examples, \n",
    "                                        labels, \n",
    "                                        16, \n",
    "                                        tokenizer,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x72b6c9bf1390>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# working with csv with datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Loading a List as a Dataset\n",
    "Suppose you have a list of dictionaries, where each dictionary represents a data point in your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "data = [\n",
    "    {'id': 1, 'text': 'Hello world', 'label': 1},\n",
    "    {'id': 2, 'text': 'HuggingFace is amazing', 'label': 0},\n",
    "    {'id': 3, 'text': 'Machine learning is fun', 'label': 1}\n",
    "]\n",
    "\n",
    "# Create a Dataset from a list of dictionaries\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Loading a Dictionary as a Dataset\n",
    "Suppose you have a dictionary where the keys represent different features (e.g., id, text, label) and the values are lists containing the data for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {\n",
    "    'id': [1, 2, 3],\n",
    "    'text': ['Hello world', 'HuggingFace is amazing', 'Machine learning is fun'],\n",
    "    'label': [1, 0, 1]\n",
    "}\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Loading a JSON File as a Dataset\n",
    "First, ensure that your JSON file is structured properly. For example, the JSON might look something like this if saved in a file named data.json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 3 examples [00:00, 399.77 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_data = Dataset.from_json(\"../data/dummy_json.json\")\n",
    "json_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Loading a CSV File as a Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset.from_csv(\"../data/dummy_csv.csv\")\n",
    "data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing columns as explained previously\n",
    "data = data.remove_columns(['id'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 793.12 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello world', 'huggingface is amazing', 'machine learning is fun']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 915.19 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uppercase Text: ['HELLO WORLD', 'HUGGINGFACE IS AMAZING', 'MACHINE LEARNING IS FUN']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3/3 [00:00<00:00, 864.86 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After adding a new column: Dataset({\n",
      "    features: ['id', 'text', 'label', 'dummy'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 3/3 [00:00<00:00, 1102.31 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'text', 'label', 'dummy'],\n",
      "    num_rows: 1\n",
      "})\n",
      "After Shuffling: ['MACHINE LEARNING IS FUN', 'HUGGINGFACE IS AMAZING', 'HELLO WORLD']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def lowercase_text(example):\n",
    "    example['text'] = example['text'].lower()\n",
    "    return example\n",
    "\n",
    "def uppercase_text(example):\n",
    "    example['text'] = example['text'].upper()\n",
    "    return example\n",
    "\n",
    "data = data.map(lowercase_text)\n",
    "\n",
    "print(data['text'])\n",
    "data = data.map(uppercase_text)\n",
    "print(\"Uppercase Text: \" + str(data['text']))\n",
    "\n",
    "def add_a_dummy_column(item):\n",
    "    item.update({'dummy': 'dummy'})\n",
    "    return item\n",
    "\n",
    "data = data.map(add_a_dummy_column)\n",
    "print(f\"After adding a new column: {data}\")\n",
    "\n",
    "print(data.filter(lambda x: 'WORLD' in x['text']))\n",
    "\n",
    "data = data.shuffle(seed=42)\n",
    "print(f\"After Shuffling: {data['text']}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 15 examples [00:00, 4736.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 12\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'text', 'label'],\n",
       "        num_rows: 3\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset.from_csv(\"../data/small_data.csv\")\n",
    "train_test_split = data.train_test_split(test_size=0.2)\n",
    "train_test_split"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vids",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
